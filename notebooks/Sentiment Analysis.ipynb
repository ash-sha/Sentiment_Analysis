{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Load Data:",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:53:19.007083Z",
     "start_time": "2024-11-29T06:53:18.989907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:53:19.613564Z",
     "start_time": "2024-11-29T06:53:19.596905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import LancasterStemmer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:35.691670Z",
     "start_time": "2024-11-29T06:49:31.470774Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Load raw data\n",
    "amazon_data = pd.read_csv('/MLOps/Sentiment/data/test_amazon.csv')\n",
    "amazon_test = pd.read_csv('/MLOps/Sentiment/data/test_amazon.csv')\n",
    "movie_data = pd.read_csv(\"/MLOps/Sentiment/data/train.csv\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:35.882486Z",
     "start_time": "2024-11-29T06:49:35.798199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "amazon_data.loc[len(amazon_data)] = amazon_data.columns\n",
    "amazon_data.columns = [\"polarity\",\"title\",'review']\n",
    "amazon_data[\"polarity\"] = amazon_data[\"polarity\"].astype(int)\n",
    "amazon_data[\"polarity\"] = amazon_data[\"polarity\"].replace({1:\"Negative\",2:\"Positive\"})\n",
    "amazon_data[\"datatype\"] = \"Train\"\n",
    "amazon_data[\"reviewtype\"] = \"Product\"\n",
    "amazon_data = amazon_data.drop(\"title\", axis=1)\n",
    "# amazon_data = pd.concat([amazon_data[amazon_data[\"polarity\"]==\"Positive\"].head(100000),amazon_data[amazon_data[\"polarity\"]==\"Negative\"].head(100000)]).reset_index(drop=True)\n",
    "amazon_data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        polarity                                             review datatype  \\\n",
       "0       Positive  Despite the fact that I have only played a sma...    Train   \n",
       "1       Negative  I bought this charger in Jul 2003 and it worke...    Train   \n",
       "2       Positive  Check out Maha Energy's website. Their Powerex...    Train   \n",
       "3       Positive  Reviewed quite a bit of the combo players and ...    Train   \n",
       "4       Negative  I also began having the incorrect disc problem...    Train   \n",
       "...          ...                                                ...      ...   \n",
       "399995  Negative  My son recieved this as a birthday gift 2 mont...    Train   \n",
       "399996  Negative  I bought this toy for my son who loves the \"Th...    Train   \n",
       "399997  Positive  This is a compilation of a wide range of Mitfo...    Train   \n",
       "399998  Negative  This DVD will be a disappointment if you get i...    Train   \n",
       "399999  Positive  My lovely Pat has one of the GREAT voices of h...    Train   \n",
       "\n",
       "       reviewtype  \n",
       "0         Product  \n",
       "1         Product  \n",
       "2         Product  \n",
       "3         Product  \n",
       "4         Product  \n",
       "...           ...  \n",
       "399995    Product  \n",
       "399996    Product  \n",
       "399997    Product  \n",
       "399998    Product  \n",
       "399999    Product  \n",
       "\n",
       "[400000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "      <th>datatype</th>\n",
       "      <th>reviewtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I also began having the incorrect disc problem...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>Negative</td>\n",
       "      <td>My son recieved this as a birthday gift 2 mont...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I bought this toy for my son who loves the \"Th...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>Positive</td>\n",
       "      <td>This is a compilation of a wide range of Mitfo...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This DVD will be a disappointment if you get i...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>Positive</td>\n",
       "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:36.008489Z",
     "start_time": "2024-11-29T06:49:35.923605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "amazon_test.loc[len(amazon_test)] = amazon_test.columns\n",
    "amazon_test.columns = [\"polarity\",\"title\",'review']\n",
    "amazon_test[\"polarity\"] = amazon_test[\"polarity\"].astype(int)\n",
    "amazon_test[\"polarity\"] = amazon_test[\"polarity\"].replace({1:\"Negative\",2:\"Positive\"})\n",
    "amazon_test[\"datatype\"] = \"Test\"\n",
    "amazon_test[\"reviewtype\"] = \"Product\"\n",
    "amazon_test = amazon_test.drop(\"title\", axis=1)\n",
    "# amazon_test = pd.concat([amazon_test[amazon_test[\"polarity\"]==\"Positive\"].head(20000),amazon_test[amazon_test[\"polarity\"]==\"Negative\"].head(20000)]).reset_index(drop=True)\n",
    "amazon_test"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        polarity                                             review datatype  \\\n",
       "0       Positive  Despite the fact that I have only played a sma...     Test   \n",
       "1       Negative  I bought this charger in Jul 2003 and it worke...     Test   \n",
       "2       Positive  Check out Maha Energy's website. Their Powerex...     Test   \n",
       "3       Positive  Reviewed quite a bit of the combo players and ...     Test   \n",
       "4       Negative  I also began having the incorrect disc problem...     Test   \n",
       "...          ...                                                ...      ...   \n",
       "399995  Negative  My son recieved this as a birthday gift 2 mont...     Test   \n",
       "399996  Negative  I bought this toy for my son who loves the \"Th...     Test   \n",
       "399997  Positive  This is a compilation of a wide range of Mitfo...     Test   \n",
       "399998  Negative  This DVD will be a disappointment if you get i...     Test   \n",
       "399999  Positive  My lovely Pat has one of the GREAT voices of h...     Test   \n",
       "\n",
       "       reviewtype  \n",
       "0         Product  \n",
       "1         Product  \n",
       "2         Product  \n",
       "3         Product  \n",
       "4         Product  \n",
       "...           ...  \n",
       "399995    Product  \n",
       "399996    Product  \n",
       "399997    Product  \n",
       "399998    Product  \n",
       "399999    Product  \n",
       "\n",
       "[400000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "      <th>datatype</th>\n",
       "      <th>reviewtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I also began having the incorrect disc problem...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>Negative</td>\n",
       "      <td>My son recieved this as a birthday gift 2 mont...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I bought this toy for my son who loves the \"Th...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>Positive</td>\n",
       "      <td>This is a compilation of a wide range of Mitfo...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This DVD will be a disappointment if you get i...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>Positive</td>\n",
       "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:36.080590Z",
     "start_time": "2024-11-29T06:49:36.038917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "movie_data = movie_data.drop(\"Unnamed: 0\", axis=1)\n",
    "movie_data.columns = [\"review\",\"polarity\"]\n",
    "movie_data[\"polarity\"] = movie_data[\"polarity\"].astype(str)\n",
    "movie_data[\"reviewtype\"] = \"Movies\"\n",
    "\n",
    "# split them equally so the case balance is preserved in split\n",
    "tta, ta = train_test_split(movie_data, test_size=0.15, stratify=movie_data['polarity'], random_state=42)\n",
    "tta['datatype'] = 'Train'\n",
    "ta['datatype'] = 'Test'\n",
    "\n",
    "movie_data_stratified = pd.concat([tta, ta]).reset_index(drop=True)\n",
    "movie_data_stratified = movie_data_stratified[[\"polarity\",\"review\",\"datatype\",\"reviewtype\"]]\n",
    "movie_data_stratified"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      polarity                                             review datatype  \\\n",
       "0          neg  No idea how this is rated as high as it is (5....    Train   \n",
       "1          neg  I just blew four dollars renting this movie! W...    Train   \n",
       "2          neg  As others have mentioned, this movie is simila...    Train   \n",
       "3          pos  Begotten is black and white distorted images. ...    Train   \n",
       "4          pos  First of all - I'm not one to go all sappy ove...    Train   \n",
       "...        ...                                                ...      ...   \n",
       "39995      pos  Paul Verhoeven's De Vierde Man (The Fourth Man...     Test   \n",
       "39996      neg  This movie could have been an impressing epic,...     Test   \n",
       "39997      pos  10/10 for this film.<br /><br />i'm a british ...     Test   \n",
       "39998      neg  First off, the title character is not even the...     Test   \n",
       "39999      pos  Coming shortly before the imposition of a mora...     Test   \n",
       "\n",
       "      reviewtype  \n",
       "0         Movies  \n",
       "1         Movies  \n",
       "2         Movies  \n",
       "3         Movies  \n",
       "4         Movies  \n",
       "...          ...  \n",
       "39995     Movies  \n",
       "39996     Movies  \n",
       "39997     Movies  \n",
       "39998     Movies  \n",
       "39999     Movies  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "      <th>datatype</th>\n",
       "      <th>reviewtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>No idea how this is rated as high as it is (5....</td>\n",
       "      <td>Train</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>I just blew four dollars renting this movie! W...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>As others have mentioned, this movie is simila...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Begotten is black and white distorted images. ...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>First of all - I'm not one to go all sappy ove...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>pos</td>\n",
       "      <td>Paul Verhoeven's De Vierde Man (The Fourth Man...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>neg</td>\n",
       "      <td>This movie could have been an impressing epic,...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>pos</td>\n",
       "      <td>10/10 for this film.&lt;br /&gt;&lt;br /&gt;i'm a british ...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>neg</td>\n",
       "      <td>First off, the title character is not even the...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>pos</td>\n",
       "      <td>Coming shortly before the imposition of a mora...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:36.331448Z",
     "start_time": "2024-11-29T06:49:36.217217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.concat([amazon_data,amazon_test,movie_data_stratified]).reset_index(drop=True)\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        polarity                                             review datatype  \\\n",
       "0       Positive  Despite the fact that I have only played a sma...    Train   \n",
       "1       Negative  I bought this charger in Jul 2003 and it worke...    Train   \n",
       "2       Positive  Check out Maha Energy's website. Their Powerex...    Train   \n",
       "3       Positive  Reviewed quite a bit of the combo players and ...    Train   \n",
       "4       Negative  I also began having the incorrect disc problem...    Train   \n",
       "...          ...                                                ...      ...   \n",
       "839995       pos  Paul Verhoeven's De Vierde Man (The Fourth Man...     Test   \n",
       "839996       neg  This movie could have been an impressing epic,...     Test   \n",
       "839997       pos  10/10 for this film.<br /><br />i'm a british ...     Test   \n",
       "839998       neg  First off, the title character is not even the...     Test   \n",
       "839999       pos  Coming shortly before the imposition of a mora...     Test   \n",
       "\n",
       "       reviewtype  \n",
       "0         Product  \n",
       "1         Product  \n",
       "2         Product  \n",
       "3         Product  \n",
       "4         Product  \n",
       "...           ...  \n",
       "839995     Movies  \n",
       "839996     Movies  \n",
       "839997     Movies  \n",
       "839998     Movies  \n",
       "839999     Movies  \n",
       "\n",
       "[840000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "      <th>datatype</th>\n",
       "      <th>reviewtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I also began having the incorrect disc problem...</td>\n",
       "      <td>Train</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839995</th>\n",
       "      <td>pos</td>\n",
       "      <td>Paul Verhoeven's De Vierde Man (The Fourth Man...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839996</th>\n",
       "      <td>neg</td>\n",
       "      <td>This movie could have been an impressing epic,...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839997</th>\n",
       "      <td>pos</td>\n",
       "      <td>10/10 for this film.&lt;br /&gt;&lt;br /&gt;i'm a british ...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839998</th>\n",
       "      <td>neg</td>\n",
       "      <td>First off, the title character is not even the...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839999</th>\n",
       "      <td>pos</td>\n",
       "      <td>Coming shortly before the imposition of a mora...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Movies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:36.595621Z",
     "start_time": "2024-11-29T06:49:36.436456Z"
    }
   },
   "cell_type": "code",
   "source": "data.groupby([\"reviewtype\",\"polarity\",\"datatype\"]).count()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                              review\n",
       "reviewtype polarity datatype        \n",
       "Movies     neg      Test        3000\n",
       "                    Train      17000\n",
       "           pos      Test        3000\n",
       "                    Train      17000\n",
       "Product    Negative Test      200000\n",
       "                    Train     200000\n",
       "           Positive Test      200000\n",
       "                    Train     200000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewtype</th>\n",
       "      <th>polarity</th>\n",
       "      <th>datatype</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Movies</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">neg</th>\n",
       "      <th>Test</th>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pos</th>\n",
       "      <th>Test</th>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Product</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Negative</th>\n",
       "      <th>Test</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Positive</th>\n",
       "      <th>Test</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split Data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:36.967569Z",
     "start_time": "2024-11-29T06:49:36.700009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text = data[data['datatype']==\"Train\"][\"review\"].tolist()\n",
    "train_labels = data[data['datatype']==\"Train\"]['polarity'].tolist()\n",
    "\n",
    "test_text = data[data['datatype']==\"Test\"][\"review\"].tolist()\n",
    "test_labels = data[data['datatype']==\"Test\"]['polarity'].tolist()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "### Text cleaning and preprocessing:",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.663574Z",
     "start_time": "2024-11-29T06:49:37.075044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the lemmatizer and stopword list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text_nltk(text_list):\n",
    "    '''Function to preprocess a list of texts by cleaning, lemmatizing, and removing unnecessary elements using NLTK.'''\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in text_list:\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [\n",
    "            lemmatizer.lemmatize(token) for token in tokens\n",
    "            if token not in stop_words               # Remove stopwords\n",
    "            and token not in string.punctuation      # Remove punctuation\n",
    "            and token.isalpha()                      # Keep only alphabetic words (no digits or symbols)\n",
    "        ]\n",
    "        \n",
    "        processed_texts.append(\" \".join(tokens))\n",
    "\n",
    "    return processed_texts\n",
    "\n",
    "# Preprocess the training and test data\n",
    "train_text_processed = preprocess_text_nltk(train_text)\n",
    "test_text_processed = preprocess_text_nltk(test_text)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m processed_texts\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Preprocess the training and test data\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m train_text_processed \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_text_nltk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m test_text_processed \u001B[38;5;241m=\u001B[39m preprocess_text_nltk(test_text)\n",
      "Cell \u001B[0;32mIn[9], line 11\u001B[0m, in \u001B[0;36mpreprocess_text_nltk\u001B[0;34m(text_list)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m text_list:\n\u001B[1;32m     10\u001B[0m     text \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39mlower()\n\u001B[0;32m---> 11\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m \u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     13\u001B[0m         lemmatizer\u001B[38;5;241m.\u001B[39mlemmatize(token) \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tokens\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m token \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words               \u001B[38;5;66;03m# Remove stopwords\u001B[39;00m\n\u001B[1;32m     15\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m token \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string\u001B[38;5;241m.\u001B[39mpunctuation      \u001B[38;5;66;03m# Remove punctuation\u001B[39;00m\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m token\u001B[38;5;241m.\u001B[39misalpha()                      \u001B[38;5;66;03m# Keep only alphabetic words (no digits or symbols)\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     ]\n\u001B[1;32m     19\u001B[0m     processed_texts\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(tokens))\n",
      "File \u001B[0;32m~/PycharmProjects/MLOps/venv/lib/python3.9/site-packages/nltk/tokenize/__init__.py:143\u001B[0m, in \u001B[0;36mword_tokenize\u001B[0;34m(text, language, preserve_line)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;124;03mReturn a tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03musing NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m:type preserve_line: bool\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    142\u001B[0m sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m sent_tokenize(text, language)\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    144\u001B[0m     token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sent)\n\u001B[1;32m    145\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/MLOps/venv/lib/python3.9/site-packages/nltk/tokenize/__init__.py:144\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;124;03mReturn a tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03musing NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m:type preserve_line: bool\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    142\u001B[0m sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m sent_tokenize(text, language)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m--> 144\u001B[0m     token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_treebank_word_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/MLOps/venv/lib/python3.9/site-packages/nltk/tokenize/destructive.py:179\u001B[0m, in \u001B[0;36mNLTKWordTokenizer.tokenize\u001B[0;34m(self, text, convert_parentheses, return_str)\u001B[0m\n\u001B[1;32m    176\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m text \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m regexp, substitution \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mENDING_QUOTES:\n\u001B[0;32m--> 179\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mregexp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubstitution\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m regexp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mCONTRACTIONS2:\n\u001B[1;32m    182\u001B[0m     text \u001B[38;5;241m=\u001B[39m regexp\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m1 \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m2 \u001B[39m\u001B[38;5;124m\"\u001B[39m, text)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py:327\u001B[0m, in \u001B[0;36m_subx\u001B[0;34m(pattern, template)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_subx\u001B[39m(pattern, template):\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001B[39;00m\n\u001B[0;32m--> 327\u001B[0m     template \u001B[38;5;241m=\u001B[39m \u001B[43m_compile_repl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtemplate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m template[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(template[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;66;03m# literal replacement\u001B[39;00m\n\u001B[1;32m    330\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m template[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vectorizer"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.668473Z",
     "start_time": "2024-11-14T12:25:53.987084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TF-IDF Vectorization\n",
    "max_feature_num = 500\n",
    "vectorizer = TfidfVectorizer(max_features=max_feature_num)"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.670421Z",
     "start_time": "2024-11-14T12:25:54.139268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit and transform training data, and transform test data\n",
    "train_vec = vectorizer.fit_transform(train_text_processed)\n",
    "test_vec = vectorizer.transform(test_text_processed)"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.687878Z",
     "start_time": "2024-11-14T12:27:15.912218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the shape of train_vec to confirm it's 2D\n",
    "print(\"Shape of train_vec:\", train_vec.shape)\n",
    "print(\"Shape of test_vec:\", test_vec.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_vec: (3634000, 500)\n",
      "Shape of test_vec: (406000, 500)\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.707460Z",
     "start_time": "2024-11-14T12:27:16.234592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Training \n",
    "clf = MultinomialNB().fit(train_vec, train_labels)"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "source": [
    "# Predict on training data\n",
    "train_pred = clf.predict(train_vec)\n",
    "\n",
    "# Predict on test data\n",
    "test_pred = clf.predict(test_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.717761Z",
     "start_time": "2024-11-14T12:27:19.028490Z"
    }
   },
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.721470Z",
     "start_time": "2024-11-14T12:27:19.516478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Accuracy\n",
    "train_accuracy = accuracy_score(train_labels, train_pred)\n",
    "test_accuracy = accuracy_score(test_labels, test_pred)"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.721854Z",
     "start_time": "2024-11-14T12:27:21.703618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Classification Report (Precision, Recall, F1-Score)\n",
    "train_classification_report = classification_report(train_labels, train_pred)\n",
    "test_classification_report = classification_report(test_labels, test_pred)"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.722050Z",
     "start_time": "2024-11-14T12:27:55.590497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Confusion Matrix\n",
    "train_confusion_matrix = confusion_matrix(train_labels, train_pred)\n",
    "test_confusion_matrix = confusion_matrix(test_labels, test_pred)"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.722211Z",
     "start_time": "2024-11-14T12:28:01.885322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the Results\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(train_classification_report)\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(test_classification_report)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7756518987341772\n",
      "Test Accuracy: 0.7735443349753695\n",
      "\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.77      0.78   1800000\n",
      "    Positive       0.77      0.78      0.78   1800000\n",
      "         neg       0.82      0.41      0.54     17000\n",
      "         pos       0.80      0.39      0.53     17000\n",
      "\n",
      "    accuracy                           0.78   3634000\n",
      "   macro avg       0.79      0.59      0.66   3634000\n",
      "weighted avg       0.78      0.78      0.78   3634000\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.77      0.77    200000\n",
      "    Positive       0.77      0.78      0.78    200000\n",
      "         neg       0.83      0.41      0.55      3000\n",
      "         pos       0.81      0.40      0.53      3000\n",
      "\n",
      "    accuracy                           0.77    406000\n",
      "   macro avg       0.80      0.59      0.66    406000\n",
      "weighted avg       0.77      0.77      0.77    406000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "source": "### Creating a Re-Usable Model called Pickle",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# save model and other necessary modules\n",
    "all_info_want_to_save = {\n",
    "    'model': clf,\n",
    "    'vectorizer': vectorizer\n",
    "}\n",
    "save_path = open(\"models/sample_trained_model.pickle\",\"wb\")\n",
    "pickle.dump(all_info_want_to_save, save_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.722387Z",
     "start_time": "2024-11-14T12:28:01.933922Z"
    }
   },
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Demonstrate"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.722786Z",
     "start_time": "2024-11-15T11:34:01.590141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load the model and vectorizer, and make predictions\n",
    "def predict_polarity(model_path, user_query):\n",
    "    saved_model_dic = pickle.load(open(model_path, \"rb\"))\n",
    "    saved_clf = saved_model_dic['model']\n",
    "    saved_vectorizer = saved_model_dic['vectorizer']\n",
    "    \n",
    "    # Preprocess the user query\n",
    "    preprocessed_query = preprocess_text_nltk([user_query])\n",
    "    \n",
    "    # Transform the query text using the saved vectorizer\n",
    "    query_vec = saved_vectorizer.transform(preprocessed_query)\n",
    "    \n",
    "    # Predict the polarity\n",
    "    prediction = saved_clf.predict(query_vec)\n",
    "    \n",
    "    return prediction[0]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:49:47.723003Z",
     "start_time": "2024-11-15T11:35:54.069437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main function to interact with the user\n",
    "def main():\n",
    "    # User input query\n",
    "    user_query = input(\"Enter your query: \")\n",
    "    \n",
    "    # Load the model and make prediction\n",
    "    polarity = predict_polarity(\"/MLOps/Sentiment/models/sample_trained_model.pickle\", user_query)\n",
    "    \n",
    "    # #Vader\n",
    "    # analyzer = SentimentIntensityAnalyzer()\n",
    "    # scores = analyzer.polarity_scores(user_query)\n",
    "\n",
    "    # Output the polarity\n",
    "    print(f\"Polarity of your query: {polarity}\")\n",
    "    # print(scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_text_nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# print(scores)\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 18\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 7\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m user_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnter your query: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Load the model and make prediction\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m polarity \u001B[38;5;241m=\u001B[39m \u001B[43mpredict_polarity\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/Users/aswathshakthi/PycharmProjects/MLOps/Sentiment Analysis CW1/models/sample_trained_model.pickle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_query\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# #Vader\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# analyzer = SentimentIntensityAnalyzer()\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# scores = analyzer.polarity_scores(user_query)\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Output the polarity\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPolarity of your query: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpolarity\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m, in \u001B[0;36mpredict_polarity\u001B[0;34m(model_path, user_query)\u001B[0m\n\u001B[1;32m      5\u001B[0m saved_vectorizer \u001B[38;5;241m=\u001B[39m saved_model_dic[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvectorizer\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Preprocess the user query\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m preprocessed_query \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_text_nltk\u001B[49m([user_query])\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Transform the query text using the saved vectorizer\u001B[39;00m\n\u001B[1;32m     11\u001B[0m query_vec \u001B[38;5;241m=\u001B[39m saved_vectorizer\u001B[38;5;241m.\u001B[39mtransform(preprocessed_query)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'preprocess_text_nltk' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
